{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PU9dJGYOVKBV"
      },
      "outputs": [],
      "source": [
        "# Building Mini-Transformer from scratch\n",
        "# this code for learning\n",
        "# Using Pytorch\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"The attention mechanism - the heart of transformers\"\"\"\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads\n",
        "\n",
        "        # Linear layers for Q, K, V\n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        batch_size, seq_len, d_model = x.shape\n",
        "\n",
        "        # 1. Create Q, K, V\n",
        "        Q = self.W_q(x)  # [batch, seq_len, d_model]\n",
        "        K = self.W_k(x)\n",
        "        V = self.W_v(x)\n",
        "\n",
        "        # 2. Split into multiple heads\n",
        "        Q = Q.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        K = K.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        V = V.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        # Now: [batch, num_heads, seq_len, d_k]\n",
        "\n",
        "        # 3. Attention calculation\n",
        "        attention_output = self._attention(Q, K, V, mask)\n",
        "\n",
        "        # 4. Concatenate heads\n",
        "        attention_output = attention_output.transpose(1, 2).contiguous().view(\n",
        "            batch_size, seq_len, d_model)\n",
        "\n",
        "        # 5. Final linear layer\n",
        "        return self.W_o(attention_output)\n",
        "\n",
        "    def _attention(self, Q, K, V, mask=None):\n",
        "        # Scaled dot-product attention\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "\n",
        "        # Apply mask if provided (for causal/decoder attention)\n",
        "        if mask is not None:\n",
        "            scores.masked_fill_(mask == 0, -1e9)\n",
        "\n",
        "        # Softmax to get attention weights\n",
        "        attention_weights = F.softmax(scores, dim=-1)\n",
        "\n",
        "        # Apply attention to values\n",
        "        return torch.matmul(attention_weights, V)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    \"\"\"Simple 2-layer MLP with ReLU\"\"\"\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear2(F.relu(self.linear1(x)))\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    \"\"\"One transformer layer: Attention + FFN + Skip connections + LayerNorm\"\"\"\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.attention = MultiHeadAttention(d_model, num_heads)\n",
        "        self.feed_forward = FeedForward(d_model, d_ff)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # Self-attention with skip connection\n",
        "        attn_output = self.attention(x, mask)\n",
        "        x = self.norm1(x + self.dropout(attn_output))  # Add & Norm\n",
        "\n",
        "        # Feed-forward with skip connection\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm2(x + self.dropout(ff_output))    # Add & Norm\n",
        "\n",
        "        return x\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"Add position information to tokens\"\"\"\n",
        "    def __init__(self, d_model, max_len=1000):\n",
        "        super().__init__()\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
        "\n",
        "        # Create sinusoidal patterns\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() *\n",
        "                           -(math.log(10000.0) / d_model))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)]\n",
        "\n",
        "class MiniTransformer(nn.Module):\n",
        "    \"\"\"Complete mini transformer model\"\"\"\n",
        "    def __init__(self, vocab_size, d_model=128, num_heads=4, num_layers=2, d_ff=512):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "\n",
        "        # Token embedding\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding(d_model)\n",
        "\n",
        "        # Transformer layers\n",
        "        self.layers = nn.ModuleList([\n",
        "            TransformerBlock(d_model, num_heads, d_ff)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        # Output head\n",
        "        self.output_projection = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # 1. Token embedding + positional encoding\n",
        "        x = self.embedding(x) * math.sqrt(self.d_model)\n",
        "        x = self.pos_encoding(x)\n",
        "\n",
        "        # 2. Pass through transformer layers\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "\n",
        "        # 3. Project to vocabulary\n",
        "        return self.output_projection(x)\n",
        "\n",
        "def create_causal_mask(seq_len):\n",
        "    \"\"\"Create mask so model can't see future tokens\"\"\"\n",
        "    mask = torch.tril(torch.ones(seq_len, seq_len))\n",
        "    return mask.unsqueeze(0).unsqueeze(0)  # Add batch and head dims\n",
        "\n",
        "# Let's test it!\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"üöÄ Testing Mini Transformer\")\n",
        "\n",
        "    # Create model\n",
        "    vocab_size = 100\n",
        "    model = MiniTransformer(vocab_size=vocab_size, d_model=64, num_heads=4, num_layers=2)\n",
        "\n",
        "    # Test input: batch of token sequences\n",
        "    batch_size = 2\n",
        "    seq_len = 10\n",
        "    x = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
        "\n",
        "    print(f\"Input shape: {x.shape}\")\n",
        "    print(f\"Input tokens: {x[0].tolist()}\")  # Show first sequence\n",
        "\n",
        "    # Create causal mask (for autoregressive generation)\n",
        "    mask = create_causal_mask(seq_len)\n",
        "\n",
        "    # Forward pass\n",
        "    with torch.no_grad():\n",
        "        output = model(x, mask)\n",
        "\n",
        "    print(f\"Output shape: {output.shape}\")  # [batch, seq_len, vocab_size]\n",
        "    print(f\"Output logits for first token: {output[0, 0, :5].tolist()}\")\n",
        "\n",
        "    # Convert to probabilities\n",
        "    probs = F.softmax(output, dim=-1)\n",
        "    print(f\"Probabilities sum to 1: {probs[0, 0].sum().item():.3f}\")\n",
        "\n",
        "    print(\"\\n‚úÖ Transformer is working!\")\n",
        "    print(\"\\nComponents built:\")\n",
        "    print(\"  ‚úì Multi-Head Attention\")\n",
        "    print(\"  ‚úì Feed Forward Network\")\n",
        "    print(\"  ‚úì Layer Normalization\")\n",
        "    print(\"  ‚úì Positional Encoding\")\n",
        "    print(\"  ‚úì Skip Connections\")\n",
        "    print(\"  ‚úì Causal Masking\")\n",
        "\n",
        "    # Show model size\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"\\nModel has {total_params:,} parameters\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EPj9RRH0pXOE",
        "outputId": "cd7388a2-9b79-49fc-952e-723a299b00d6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Testing Mini Transformer\n",
            "Input shape: torch.Size([2, 10])\n",
            "Input tokens: [30, 76, 24, 64, 66, 3, 55, 40, 68, 68]\n",
            "Output shape: torch.Size([2, 10, 100])\n",
            "Output logits for first token: [-0.6635676622390747, -0.3851340115070343, -0.7031242847442627, 0.002661745995283127, 0.708402693271637]\n",
            "Probabilities sum to 1: 1.000\n",
            "\n",
            "‚úÖ Transformer is working!\n",
            "\n",
            "Components built:\n",
            "  ‚úì Multi-Head Attention\n",
            "  ‚úì Feed Forward Network\n",
            "  ‚úì Layer Normalization\n",
            "  ‚úì Positional Encoding\n",
            "  ‚úì Skip Connections\n",
            "  ‚úì Causal Masking\n",
            "\n",
            "Model has 178,916 parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üöÄ Mini Transformer from Scratch ‚Äî Documentation\n",
        "\n",
        "This notebook builds a **mini Transformer model** step by step in PyTorch.  \n",
        "It is a simplified version of what powers large models like GPT, BERT, etc.\n",
        "\n",
        "---\n",
        "\n",
        "## üîë Components\n",
        "\n",
        "### 1. Embedding Layer\n",
        "- Converts token IDs (integers) into dense vectors (`d_model` dimensions).\n",
        "- Provides the model with a numerical representation of each token.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Positional Encoding\n",
        "- Transformers don‚Äôt inherently know the order of tokens.  \n",
        "- Positional encoding adds **sinusoidal patterns** (sine/cosine functions) to embeddings.\n",
        "- This allows the model to understand sequence order (e.g., word #1 comes before word #2).\n",
        "\n",
        "---\n",
        "\n",
        "### 3. Multi-Head Self-Attention\n",
        "- **Core mechanism of transformers.**\n",
        "- Each token creates three vectors:\n",
        "  - **Query (Q):** What am I looking for?\n",
        "  - **Key (K):** What information do I have?\n",
        "  - **Value (V):** The actual content to pass along.\n",
        "- Formula:  \n",
        "  \\[\n",
        "  Attention(Q,K,V) = softmax\\Big(\\frac{QK^T}{\\sqrt{d_k}}\\Big)V\n",
        "  \\]\n",
        "- **Multi-heads**: run this process multiple times in parallel to capture different relationships (syntax, meaning, etc.).\n",
        "- Produces weighted combinations of values, depending on how much attention each token pays to others.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. Feed Forward Network (FFN)\n",
        "- A simple 2-layer MLP applied to each token independently.\n",
        "- Expands dimensionality (`d_model ‚Üí d_ff ‚Üí d_model`).\n",
        "- Adds non-linearity and further processing after attention.\n",
        "\n",
        "---\n",
        "\n",
        "### 5. Transformer Block\n",
        "- Combines **self-attention** and **feed-forward network**.\n",
        "- Includes:\n",
        "  - **Skip connections (residuals):** help training stability.\n",
        "  - **Layer Normalization:** keeps values stable.\n",
        "  - **Dropout:** prevents overfitting.\n",
        "\n",
        "Each block = `Attention ‚Üí Add+Norm ‚Üí FeedForward ‚Üí Add+Norm`.\n",
        "\n",
        "---\n",
        "\n",
        "### 6. Stacked Transformer Layers\n",
        "- Multiple blocks are stacked (`num_layers`) to build depth.\n",
        "- Each layer refines the representation further.\n",
        "\n",
        "---\n",
        "\n",
        "### 7. Output Projection\n",
        "- Final linear layer projects hidden states back to **vocabulary size**.\n",
        "- Produces logits ‚Üí probabilities over possible next tokens.\n",
        "\n",
        "---\n",
        "\n",
        "### 8. Causal Mask\n",
        "- Used in autoregressive models (like GPT).\n",
        "- Ensures each token only attends to **previous tokens** (not the future).\n",
        "- Implemented with a lower-triangular matrix.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚öôÔ∏è Workflow of the Mini Transformer\n",
        "1. Input tokens ‚Üí Embedding layer.  \n",
        "2. Add positional encoding.  \n",
        "3. Pass through multiple Transformer blocks:  \n",
        "   - Multi-head self-attention.  \n",
        "   - Feed-forward network.  \n",
        "   - Skip connections + normalization.  \n",
        "4. Output projection ‚Üí logits for vocabulary.  \n",
        "5. Apply softmax ‚Üí probabilities for next token prediction.  \n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Why This Matters\n",
        "- This is a **toy version of GPT**.  \n",
        "- By increasing `d_model`, `num_heads`, and `num_layers`, you approach the architecture of real LLMs.  \n",
        "- Understanding this foundation gives you the tools to build and experiment with language models.  \n"
      ],
      "metadata": {
        "id": "xgthk7Q2rc7M"
      }
    }
  ]
}